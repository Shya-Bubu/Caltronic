{
    "id": "iterative-vs-direct-methods-flashcards",
    "cards": [
        {
            "id": "fc-1",
            "front": "Direct Method Definition",
            "back": "Computes exact solution in finite, predetermined steps\n\nExample: LU factorization\n• A = LU (factor once)\n• Ly = b (forward substitution)\n• Ux = y (backward substitution)\n\nAlways works for non-singular A",
            "difficultyLevel": 1
        },
        {
            "id": "fc-2",
            "front": "Iterative Method Definition",
            "back": "Successively improves approximate solution:\nx^(k+1) = f(x^(k))\n\nStop when ||x^(k+1) - x^(k)|| < tolerance\n\nExamples: Jacobi, Gauss-Seidel, CG\n\nMay not converge for all matrices!",
            "difficultyLevel": 1
        },
        {
            "id": "fc-3",
            "front": "Jacobi Iteration Formula",
            "back": "x_i^(k+1) = (b_i - Σ A_ij x_j^(k)) / A_ii\n         for j ≠ i\n\nKey: Uses only OLD values from iteration k\n\nConverges if: Strictly diagonally dominant\n|A_ii| > Σ|A_ij| for j ≠ i",
            "difficultyLevel": 1
        },
        {
            "id": "fc-4",
            "front": "Gauss-Seidel vs Jacobi",
            "back": "Gauss-Seidel uses UPDATED values immediately:\n\nx_i^(k+1) uses x_j^(k+1) for j < i (new values)\n            and x_j^(k) for j > i (old values)\n\nTypically converges 2× faster than Jacobi\nSame convergence conditions",
            "difficultyLevel": 1
        },
        {
            "id": "fc-5",
            "front": "Spectral Radius Convergence Condition",
            "back": "Iteration x^(k+1) = Bx^(k) + c converges\n⟺ ρ(B) < 1\n\nwhere ρ(B) = max |λ_i| (largest eigenvalue magnitude)\n\nError after k iterations: ||e^(k)|| ≤ ρ^k ||e^(0)||",
            "difficultyLevel": 1
        },
        {
            "id": "fc-6",
            "front": "CG Requirements and Rate",
            "back": "Conjugate Gradient requires:\n• Symmetric matrix (A = A^T)\n• Positive definite (x^T A x > 0)\n\nConvergence:\n||e_k|| ≤ 2((√κ-1)/(√κ+1))^k ||e_0||\n\nIterations ≈ (√κ/2) ln(2/ε)\n\nwhere κ = λ_max/λ_min",
            "difficultyLevel": 1
        },
        {
            "id": "fc-7",
            "front": "Preconditioning Purpose",
            "back": "Solve M^{-1}Ax = M^{-1}b instead of Ax = b\n\nGoal: κ(M^{-1}A) << κ(A)\n\nIdeal: M = A (perfect, but expensive)\nPractical: M ≈ A but easy to invert\n\nCommon choices: ILU, SSOR, AMG",
            "difficultyLevel": 1
        },
        {
            "id": "fc-8",
            "front": "Direct vs Iterative: Memory",
            "back": "Direct (LU):\n• Must store L and U factors\n• Fill-in increases memory\n• O(fill) for sparse, O(n²) for dense\n\nIterative:\n• Only need A (not modified)\n• Plus a few vectors O(n)\n• Much lower memory!",
            "difficultyLevel": 1
        },
        {
            "id": "fc-9",
            "front": "Multiple Right-Hand Sides",
            "back": "Solving Ax = b for many different b:\n\nDirect: Factor once O(n³), solve each O(n²)\n• Efficient for many RHS\n\nIterative: Full solve for each\n• No reuse benefit\n\n→ For many RHS, direct often wins",
            "difficultyLevel": 1
        },
        {
            "id": "fc-10",
            "front": "When to Use CG vs Direct",
            "back": "Use CG:\n• Large n (> 10,000)\n• SPD matrix\n• Good preconditioner available\n• Memory constrained\n• Approximate answer OK\n\nUse Direct:\n• Small/medium n\n• Multiple RHS\n• Non-SPD matrix\n• Exact answer needed",
            "difficultyLevel": 1
        }
    ]
}